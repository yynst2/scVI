{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced autotune tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER: Most experiments in this notebook require one or more GPUs to keep their runtime a matter of hours.**\n",
    "**DISCLAIMER: To use our new autotune feature in parallel mode, you need to install [MongoDb](https://docs.mongodb.com/manual/installation/) first.**\n",
    "\n",
    "In this notebook, we give an in-depth tutorial on `scVI`'s new `autotune` module.\n",
    "\n",
    "Overall, the new module enables users to perform parallel hyperparemter search for any scVI model and on any number of GPUs/CPUs. Although, the search may be performed sequentially using only one GPU/CPU, we will focus on the paralel case.\n",
    "Note that GPUs provide a much faster approach as they are particularly suitable for neural networks gradient back-propagation.\n",
    "\n",
    "Additionally, we provide the code used to generate the results presented in our [Hyperoptimization blog post](https://yoseflab.github.io/2019/07/05/Hyperoptimization/). For an in-depth analysis of the results obtained on three gold standard scRNAseq datasets (Cortex, PBMC and BrainLarge), please to the above blog post. In the blog post, we also suggest guidelines on how and when to use our auto-tuning feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galen/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import scanpy\n",
    "import anndata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hyperopt import hp\n",
    "\n",
    "import scvi\n",
    "# from scvi.dataset import BrainLargeDataset, CortexDataset, LoomDataset, PbmcDataset\n",
    "from scvi.dataset import cortex, pbmc_dataset, brainlarge_dataset\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.inference.autotune import auto_tune_scvi_model\n",
    "from scvi.models import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"scvi.inference.autotune\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_notebook_for_test():\n",
    "    print(\"Testing the autotune advanced notebook\")\n",
    "\n",
    "# test_mode = True\n",
    "test_mode = False\n",
    "\n",
    "\n",
    "def if_not_test_else(x, y):\n",
    "    if not test_mode:\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "\n",
    "save_path = \"data/\"\n",
    "n_epochs = if_not_test_else(1000, 1)\n",
    "n_epochs_brain_large = if_not_test_else(50, 1)\n",
    "max_evals = if_not_test_else(100, 1)\n",
    "reserve_timeout = if_not_test_else(180, 5)\n",
    "fmin_timeout = if_not_test_else(300, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of principled simplicity, we provide an all-default approach to hyperparameter search for any `scVI` model.\n",
    "The few lines below present an example of how to perform hyper-parameter search for `scVI` on the Cortex dataset.\n",
    "\n",
    "Note that, by default, the model used is `scVI`'s `VAE` and the trainer is the `UnsupervisedTrainer`\n",
    "\n",
    "Also, the default search space is as follows:\n",
    "* `n_latent`: [5, 15]\n",
    "* `n_hidden`: {64, 128, 256}\n",
    "* `n_layers`: [1, 5]\n",
    "* `dropout_rate`: {0.1, 0.3, 0.5, 0.7}\n",
    "* `reconstruction_loss`: {\"zinb\", \"nb\"}\n",
    "* `lr`: {0.01, 0.005, 0.001, 0.0005, 0.0001}\n",
    "\n",
    "On a more practical note, verbosity varies in the following way:\n",
    "* `logger.setLevel(logging.WARNING)` will show a progress bar.\n",
    "* `logger.setLevel(logging.INFO)` will show global logs including the number of jobs done.\n",
    "* `logger.setLevel(logging.DEBUG)` will show detailed logs for each training (e.g the parameters tested).\n",
    "\n",
    "This function's behaviour can be customized, please refer to the rest of this tutorial as well as its documentation for information about the different parameters available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the hyperoptimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:48:16,395] INFO - scvi.dataset._utils | File /Users/galen/scVI/tests/notebooks/data/expression.bin already downloaded\n",
      "[2020-07-14 12:48:16,397] INFO - scvi.dataset.cortex | Loading Cortex data from /Users/galen/scVI/tests/notebooks/data/expression.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming to str index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:48:24,686] INFO - scvi.dataset.cortex | Finished loading Cortex data\n",
      "[2020-07-14 12:48:25,554] INFO - scvi.dataset._anndata | Using data from adata.X\n",
      "[2020-07-14 12:48:25,555] INFO - scvi.dataset._anndata | No batch_key inputted, assuming all cells are same batch\n",
      "[2020-07-14 12:48:25,559] INFO - scvi.dataset._anndata | Using labels from adata.obs[\"labels\"]\n",
      "[2020-07-14 12:48:25,560] INFO - scvi.dataset._anndata | Computing library size prior per batch\n",
      "[2020-07-14 12:48:26,163] INFO - scvi.dataset._anndata | Successfully registered anndata object containing 3005 cells, 19972 genes, and 1 batches \n",
      "Registered keys:['X', 'batch_indices', 'local_l_mean', 'local_l_var', 'labels']\n"
     ]
    }
   ],
   "source": [
    "cortex = scvi.dataset.cortex(save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:48:26,169] INFO - scvi.inference.autotune.all | Starting experiment: cortex_dataset\n",
      "[2020-07-14 12:48:26,171] DEBUG - scvi.inference.autotune.all | Using default parameter search space.\n",
      "[2020-07-14 12:48:26,172] DEBUG - scvi.inference.autotune.all | Adding default early stopping behaviour.\n",
      "[2020-07-14 12:48:26,174] INFO - scvi.inference.autotune.all | Fixed parameters: \n",
      "model: \n",
      "{}\n",
      "trainer: \n",
      "{'early_stopping_kwargs': {'early_stopping_metric': 'elbo', 'save_best_state_metric': 'elbo', 'patience': 50, 'threshold': 0, 'reduce_lr_on_plateau': True, 'lr_patience': 25, 'lr_factor': 0.2}, 'metrics_to_monitor': ['elbo']}\n",
      "train method: \n",
      "{'n_epochs': 1}\n",
      "[2020-07-14 12:48:26,174] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-14 12:48:26,188] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-14 12:48:26,209] ERROR - scvi.inference.autotune.all | Caught (2, \"No such file or directory: 'mongod'\") in auto_tune_scvi_model, starting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 147, in decorated\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 471, in auto_tune_scvi_model\n",
      "    multiple_hosts=multiple_hosts,\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 640, in _auto_tune_parallel\n",
      "    stdout=mongo_logfile,\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mongod': 'mongod'\n",
      "[2020-07-14 12:48:26,213] INFO - scvi.inference.autotune.all | Cleaning up\n",
      "[2020-07-14 12:48:26,214] DEBUG - scvi.inference.autotune.all | Cleaning up: closing files.\n",
      "[2020-07-14 12:48:26,215] DEBUG - scvi.inference.autotune.all | Cleaning up: closing queues.\n",
      "[2020-07-14 12:48:26,216] DEBUG - scvi.inference.autotune.all | Cleaning up: setting cleanup_event and joining threads.\n",
      "[2020-07-14 12:48:26,217] DEBUG - scvi.inference.autotune.all | Cleaning up: terminating processes.\n",
      "[2020-07-14 12:48:26,218] DEBUG - scvi.inference.autotune.all | Cleaning up: removing added logging handler.\n",
      "[2020-07-14 12:48:26,220] DEBUG - scvi.inference.autotune.all | Cleaning up: removing autotune FileHandler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1475, in _monitor\n",
      "    record = self.dequeue(True)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1424, in dequeue\n",
      "    return self.queue.get(block)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mongod': 'mongod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb9feb949bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreserve_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             logger_all.exception(\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mauto_tune_scvi_model\u001b[0;34m(exp_key, gene_dataset, delayed_populating, custom_objective_hyperopt, objective_kwargs, model_class, trainer_class, metric_name, metric_kwargs, posterior_name, model_specific_kwargs, trainer_specific_kwargs, train_func_specific_kwargs, space, max_evals, train_best, pickle_result, save_path, use_batches, parallel, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mmongo_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mdb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         )\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;34m\"--port={port}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         ],\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_logfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     \u001b[0;31m# let mongo server start and check it did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mongod': 'mongod'"
     ]
    }
   ],
   "source": [
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=cortex,\n",
    "    parallel=True,\n",
    "    exp_key=\"cortex_dataset\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returned objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trials` object contains detailed information about each run.\n",
    "`trials.trials` is an `Iterable` in which each element corresponds to a single run. It can be used as a dictionary for wich the key \"result\" yields a dictionnary containing the outcome of the run as defined in our default objective function (or the user's custom version). For example, it will contain information on the hyperparameters used (under the \"space\" key), the resulting metric (under the \"loss\" key) or the status of the run.\n",
    "\n",
    "The `best_trainer` object can be used directly as an scVI `Trainer` object. It is the result of a training on the whole dataset provided using the optimal set of hyperparameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom hyperamater space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our default can be a good one in a number of cases, we still provide an easy way to use custom values for the hyperparameters search space.\n",
    "These are broken down in three categories:\n",
    "* Hyperparameters for the `Trainer` instance. (if any)\n",
    "* Hyperparameters for the `Trainer` instance's `train` method. (e.g `lr`)\n",
    "* Hyperparameters for the model instance. (e.g `n_layers`)\n",
    "\n",
    "To build your own hyperparameter space follow the scheme used in `scVI`'s codebase as well as the sample below.\n",
    "Note the various spaces you define, have to follow the `hyperopt` syntax, for which you can find a detailed description [here](https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space).\n",
    "\n",
    "For example, if you were to want to search over a continuous range of droupouts varying in [0.1, 0.3] and for a continuous learning rate varying in [0.001, 0.0001], you could use the following search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:48:37,149] INFO - scvi.inference.autotune.all | Starting experiment: cortex_dataset_custom_space\n",
      "[2020-07-14 12:48:37,150] DEBUG - scvi.inference.autotune.all | Adding default early stopping behaviour.\n",
      "[2020-07-14 12:48:37,151] INFO - scvi.inference.autotune.all | Fixed parameters: \n",
      "model: \n",
      "{}\n",
      "trainer: \n",
      "{'early_stopping_kwargs': {'early_stopping_metric': 'elbo', 'save_best_state_metric': 'elbo', 'patience': 50, 'threshold': 0, 'reduce_lr_on_plateau': True, 'lr_patience': 25, 'lr_factor': 0.2}, 'metrics_to_monitor': ['elbo']}\n",
      "train method: \n",
      "{'n_epochs': 1}\n",
      "[2020-07-14 12:48:37,152] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-14 12:48:37,154] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-14 12:48:37,168] ERROR - scvi.inference.autotune.all | Caught (2, \"No such file or directory: 'mongod'\") in auto_tune_scvi_model, starting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 147, in decorated\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 471, in auto_tune_scvi_model\n",
      "    multiple_hosts=multiple_hosts,\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 640, in _auto_tune_parallel\n",
      "    stdout=mongo_logfile,\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mongod': 'mongod'\n",
      "[2020-07-14 12:48:37,172] INFO - scvi.inference.autotune.all | Cleaning up\n",
      "[2020-07-14 12:48:37,173] DEBUG - scvi.inference.autotune.all | Cleaning up: closing files.\n",
      "[2020-07-14 12:48:37,176] DEBUG - scvi.inference.autotune.all | Cleaning up: closing queues.\n",
      "[2020-07-14 12:48:37,178] DEBUG - scvi.inference.autotune.all | Cleaning up: setting cleanup_event and joining threads.\n",
      "[2020-07-14 12:48:37,181] DEBUG - scvi.inference.autotune.all | Cleaning up: terminating processes.\n",
      "[2020-07-14 12:48:37,182] DEBUG - scvi.inference.autotune.all | Cleaning up: removing added logging handler.\n",
      "[2020-07-14 12:48:37,184] DEBUG - scvi.inference.autotune.all | Cleaning up: removing autotune FileHandler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1475, in _monitor\n",
      "    record = self.dequeue(True)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1424, in dequeue\n",
      "    return self.queue.get(block)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mongod': 'mongod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c272202d0f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreserve_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             logger_all.exception(\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mauto_tune_scvi_model\u001b[0;34m(exp_key, gene_dataset, delayed_populating, custom_objective_hyperopt, objective_kwargs, model_class, trainer_class, metric_name, metric_kwargs, posterior_name, model_specific_kwargs, trainer_specific_kwargs, train_func_specific_kwargs, space, max_evals, train_best, pickle_result, save_path, use_batches, parallel, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mmongo_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mdb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         )\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;34m\"--port={port}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         ],\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_logfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     \u001b[0;31m# let mongo server start and check it did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mongod': 'mongod'"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    \"model_tunable_kwargs\": {\"dropout_rate\": hp.uniform(\"dropout_rate\", 0.1, 0.3)},\n",
    "    \"train_func_tunable_kwargs\": {\"lr\": hp.loguniform(\"lr\", -4.0, -3.0)},\n",
    "}\n",
    "\n",
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=cortex,\n",
    "    space=space,\n",
    "    parallel=True,\n",
    "    exp_key=\"cortex_dataset_custom_space\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom objective metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, our autotune process tracks the marginal negative log likelihood of the best state of the model according ot the held-out Evidence Lower BOund (ELBO). But, if you want to track a different early stopping metric and optimize a different loss you can use `auto_tune_scvi_model`'s parameters.\n",
    "\n",
    "For example, if for some reason, you had a dataset coming from two batches (i.e two merged datasets) and wanted to optimize the hyperparameters for the batch mixing entropy. You could use the code below, which makes use of the `metric_name` argument of `auto_tune_scvi_model`. This can work for any metric that is implemented in the  `Posterior` class you use. You may also specify the name of the `Posterior` attribute you want to use (e.g \"train_set\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:48:42,028] INFO - scvi.dataset._utils | File data/gene_info_pbmc.csv already downloaded\n",
      "[2020-07-14 12:48:42,029] INFO - scvi.dataset._utils | File data/pbmc_metadata.pickle already downloaded\n",
      "[2020-07-14 12:48:42,080] INFO - scvi.dataset._utils | File data/pbmc8k/filtered_gene_bc_matrices.tar.gz already downloaded\n",
      "[2020-07-14 12:48:42,082] INFO - scvi.dataset.dataset10X | Extracting tar file\n",
      "[2020-07-14 12:49:00,757] INFO - scvi.dataset.dataset10X | Removing extracted data at data/pbmc8k/filtered_gene_bc_matrices\n",
      "[2020-07-14 12:49:01,171] INFO - scvi.dataset._utils | File data/pbmc4k/filtered_gene_bc_matrices.tar.gz already downloaded\n",
      "[2020-07-14 12:49:01,173] INFO - scvi.dataset.dataset10X | Extracting tar file\n",
      "[2020-07-14 12:49:10,146] INFO - scvi.dataset.dataset10X | Removing extracted data at data/pbmc4k/filtered_gene_bc_matrices\n",
      "[2020-07-14 12:49:11,460] INFO - scvi.dataset._anndata | Using data from adata.X\n",
      "[2020-07-14 12:49:11,461] INFO - scvi.dataset._anndata | Using batches from adata.obs[\"batch\"]\n",
      "[2020-07-14 12:49:11,462] INFO - scvi.dataset._anndata | Using labels from adata.obs[\"labels\"]\n",
      "[2020-07-14 12:49:11,463] INFO - scvi.dataset._anndata | Computing library size prior per batch\n",
      "[2020-07-14 12:49:11,558] INFO - scvi.dataset._anndata | Successfully registered anndata object containing 11990 cells, 3346 genes, and 2 batches \n",
      "Registered keys:['X', 'batch_indices', 'local_l_mean', 'local_l_var', 'labels']\n"
     ]
    }
   ],
   "source": [
    "pbmc = pbmc_dataset(save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-14 12:49:11,581] INFO - scvi.inference.autotune.all | Starting experiment: pbmc_entropy_batch_mixing\n",
      "[2020-07-14 12:49:11,582] DEBUG - scvi.inference.autotune.all | Using default parameter search space.\n",
      "[2020-07-14 12:49:11,584] DEBUG - scvi.inference.autotune.all | Adding default early stopping behaviour.\n",
      "[2020-07-14 12:49:11,586] INFO - scvi.inference.autotune.all | Fixed parameters: \n",
      "model: \n",
      "{}\n",
      "trainer: \n",
      "{'early_stopping_kwargs': {'early_stopping_metric': 'elbo', 'save_best_state_metric': 'elbo', 'patience': 50, 'threshold': 0, 'reduce_lr_on_plateau': True, 'lr_patience': 25, 'lr_factor': 0.2}, 'metrics_to_monitor': ['elbo']}\n",
      "train method: \n",
      "{'n_epochs': 1}\n",
      "[2020-07-14 12:49:11,587] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-14 12:49:11,589] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-14 12:49:11,607] ERROR - scvi.inference.autotune.all | Caught (2, \"No such file or directory: 'mongod'\") in auto_tune_scvi_model, starting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 147, in decorated\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 471, in auto_tune_scvi_model\n",
      "    multiple_hosts=multiple_hosts,\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 640, in _auto_tune_parallel\n",
      "    stdout=mongo_logfile,\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mongod': 'mongod'\n",
      "[2020-07-14 12:49:11,613] INFO - scvi.inference.autotune.all | Cleaning up\n",
      "[2020-07-14 12:49:11,614] DEBUG - scvi.inference.autotune.all | Cleaning up: closing files.\n",
      "[2020-07-14 12:49:11,615] DEBUG - scvi.inference.autotune.all | Cleaning up: closing queues.\n",
      "[2020-07-14 12:49:11,617] DEBUG - scvi.inference.autotune.all | Cleaning up: setting cleanup_event and joining threads.\n",
      "[2020-07-14 12:49:11,618] DEBUG - scvi.inference.autotune.all | Cleaning up: terminating processes.\n",
      "[2020-07-14 12:49:11,620] DEBUG - scvi.inference.autotune.all | Cleaning up: removing added logging handler.\n",
      "[2020-07-14 12:49:11,621] DEBUG - scvi.inference.autotune.all | Cleaning up: removing autotune FileHandler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1475, in _monitor\n",
      "    record = self.dequeue(True)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1424, in dequeue\n",
      "    return self.queue.get(block)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mongod': 'mongod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2bb90398cb13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mreserve_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             logger_all.exception(\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mauto_tune_scvi_model\u001b[0;34m(exp_key, gene_dataset, delayed_populating, custom_objective_hyperopt, objective_kwargs, model_class, trainer_class, metric_name, metric_kwargs, posterior_name, model_specific_kwargs, trainer_specific_kwargs, train_func_specific_kwargs, space, max_evals, train_best, pickle_result, save_path, use_batches, parallel, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mmongo_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mdb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         )\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;34m\"--port={port}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         ],\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_logfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     \u001b[0;31m# let mongo server start and check it did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mongod': 'mongod'"
     ]
    }
   ],
   "source": [
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=pbmc,\n",
    "    metric_name=\"entropy_batch_mixing\",\n",
    "    posterior_name=\"train_set\",\n",
    "    parallel=True,\n",
    "    exp_key=\"pbmc_entropy_batch_mixing\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we describe, using one of our Synthetic dataset, how to tune our annotation model `SCANVI` for, e.g, better accuracy on a 20% subset of the labelled data. Note that the model is trained in a semi-supervised framework, that is why we have a labelled and unlabelled dataset. Please, refer to the original [paper](https://www.biorxiv.org/content/10.1101/532895v1) for details on SCANVI!\n",
    "\n",
    "In this case, as described in our `annotation` notebook we may want to form the labelled/unlabelled sets using batch indices. Unfortunately, that requires a little \"by hand\" work. Even in that case, we are able to leverage the new autotune module to perform hyperparameter tuning. In order to do so, one has to write his own objective function and feed it to `auto_tune_scvi_model`.\n",
    "\n",
    "One can proceed as described below.\n",
    "Note three important conditions:\n",
    "* Since it is going to be pickled the objective should not be implemented in the \"__main__\" module, i.e an executable script or a notebook.\n",
    "* the objective should have the search space as its first attribute and a boolean `is_best_training` as its second.\n",
    "* If not using a cutstom search space, it should be expected to take the form of a dictionary with the following keys:\n",
    "    * `\"model_tunable_kwargs\"`\n",
    "    * `\"trainer_tunable_kwargs\"`\n",
    "    * `\"train_func_tunable_kwargs\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from scvi.dataset.loom import read_loom_via_url\n",
    "# import os\n",
    "# filename=\"simulation_1.loom\"\n",
    "# save_path=os.path.join(save_path, \"simulation/\")\n",
    "# url=\"https://github.com/YosefLab/scVI-data/raw/master/simulation/simulation_1.loom\"\n",
    "\n",
    "# a = read_loom_via_url(url, filename, save_path)\n",
    "# backup_url=\"https://github.com/YosefLab/scVI-data/raw/master/simulation/simulation_1.h5ad\"\n",
    "# synthetic_dataset = scanpy.read(os.path.join(save_path, \"simulation/simulation_1.h5ad\"), backup_url = backup_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.autotune_advanced_notebook import custom_objective_hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective_kwargs = dict(dataset=synthetic_dataset, n_epochs=n_epochs)\n",
    "# best_trainer, trials = auto_tune_scvi_model(\n",
    "#     custom_objective_hyperopt=custom_objective_hyperopt,\n",
    "#     objective_kwargs=objective_kwargs,\n",
    "#     parallel=True,\n",
    "#     exp_key=\"synthetic_dataset_scanvi\",\n",
    "#     max_evals=max_evals,\n",
    "#     reserve_timeout=reserve_timeout,\n",
    "#     fmin_timeout=fmin_timeout,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed populating, for very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER: We don't actually need this for the BrainLarge dataset with 720 genes, this is just an example.**\n",
    "\n",
    "The fact is that after building the objective function and feeding it to `hyperopt`, it is pickled on to the `MongoWorkers`. Thus, if you pass a loaded dataset as a partial argument to the objective function, and this dataset exceeds 4Gb, you'll get a `PickleError` (Objects larger than 4Gb can't be pickled).\n",
    "\n",
    "To remedy this issue, in case you have a very large dataset for which you want to perform hyperparameter optimization, you should subclass `scVI`'s `DownloadableDataset` or use one of its many existing subclasses, such that the dataset can be populated inside the objective function which is called by each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    brain_large_dataset_path = '../data/brainlarge_dataset_test.h5ad'\n",
    "else:\n",
    "    #brain_large_dataset_path = path_to_processed_brain_large\n",
    "    pass\n",
    "\n",
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=brain_large_dataset_path,\n",
    "    delayed_populating=True,\n",
    "    parallel=True,\n",
    "    exp_key=\"brain_large_dataset\",\n",
    "    max_evals=max_evals,\n",
    "    trainer_specific_kwargs={\n",
    "        \"early_stopping_kwargs\": {\n",
    "            \"early_stopping_metric\": \"elbo\",\n",
    "            \"save_best_state_metric\": \"elbo\",\n",
    "            \"patience\": 20,\n",
    "            \"threshold\": 0,\n",
    "            \"reduce_lr_on_plateau\": True,\n",
    "            \"lr_patience\": 10,\n",
    "            \"lr_factor\": 0.2,\n",
    "        }\n",
    "    },\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs_brain_large},\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog post reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we provide some code to reproduce the results of our [blog post](https://yoseflab.github.io/2019/07/05/Hyperoptimization/) on hyperparameter search with scVI.\n",
    "Note, that this can also be used as a tutorial on how to make senss of the output of the autotuning process, the `Trials` object.\n",
    "\n",
    "In `scvi` version 1.0, we removed the dataset corruption feature, so please use an earlier version of scVI for full reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cortex, Pbmc and BrainLarge hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we run the default hyperparameter optimization procedure (default search space, 100 runs) on each of the three dataset of our study:\n",
    "* The Cortex dataset (done above)\n",
    "* The Pbmc dataset\n",
    "* The Brain Large dataset (done above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=pbmc,\n",
    "    parallel=True,\n",
    "    exp_key=\"pbmc_dataset\",\n",
    "    max_evals=max_evals,\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handy class to handle the results of each experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the helper, `autotune_advanced_notebook.py` we have implemented a `Benchmarkable` class which will help with things such as benchmark computation, results visualisation in dataframes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.autotune_advanced_notebook import Benchmarkable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make experiment benchmarkables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use our helper class to store and process the results of the experiments.\n",
    "It allows us to generate:\n",
    "* Imputed values from scVI\n",
    "* Dataframes containing:\n",
    "    * For each dataset, the results of each trial along with the parameters used.\n",
    "    * For all dataset, the best result and the associated hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex = Benchmarkable(\n",
    "    global_path=results_path, exp_key=\"cortex_dataset\", name=\"Cortex tuned\"\n",
    ")\n",
    "pbmc = Benchmarkable(\n",
    "    global_path=results_path, exp_key=\"pbmc_dataset\", name=\"Pbmc tuned\"\n",
    ")\n",
    "brain_large = Benchmarkable(\n",
    "    global_path=results_path, exp_key=\"brain_large_dataset\", name=\"Brain Large tuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train each VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_one_shot = if_not_test_else(400, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = cortex.uns['scvi_summary_stats']\n",
    "vae = VAE(stats['n_genes'], n_batch=stats['n_batch']* False)\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae, cortex, train_size=0.75, use_cuda=True, frequency=1\n",
    ")\n",
    "trainer.train(n_epochs=n_epochs_one_shot, lr=1e-3)\n",
    "with open(\"trainer_cortex_one_shot\", \"wb\") as f:\n",
    "    pickle.dump(trainer, f)\n",
    "with open(\"model_cortex_one_shot\", \"wb\") as f:\n",
    "    torch.save(vae, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pbmc.uns['scvi_summary_stats']\n",
    "vae = VAE(stats['n_genes'], n_batch=stats['n_batch'] * False)\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae, pbmc, train_size=0.75, use_cuda=True, frequency=1\n",
    ")\n",
    "trainer.train(n_epochs=n_epochs_one_shot, lr=1e-3)\n",
    "with open(\"trainer_pbmc_one_shot\", \"wb\") as f:\n",
    "    pickle.dump(trainer, f)\n",
    "with open(\"model_pbmc_one_shot\", \"wb\") as f:\n",
    "    torch.save(vae, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    brainlarge = anndata.read(\"../data/brainlarge_dataset_test.h5ad\")\n",
    "else:\n",
    "    brainlarge = brainlarge_dataset()\n",
    "stats = brainlarge.uns['scvi_summary_stats']\n",
    "\n",
    "vae = VAE(stats['n_genes'], n_batch=stats['n_batch'] * False)\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae, brain_large_dataset, train_size=0.75, use_cuda=True, frequency=1\n",
    ")\n",
    "trainer.train(n_epochs=n_epochs_brain_large, lr=1e-3)\n",
    "with open(\"trainer_brain_large_one_shot\", \"wb\") as f:\n",
    "    pickle.dump(trainer, f)\n",
    "with open(\"model_brain_large_one_shot\", \"wb\") as f:\n",
    "    torch.save(vae, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use our helper class to contain, preprocess and access the results of each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_one_shot = Benchmarkable(\n",
    "    trainer_fname=\"trainer_cortex_one_shot\",\n",
    "    model_fname=\"model_cortex_one_shot\",\n",
    "    name=\"Cortex default\",\n",
    "    is_one_shot=True,\n",
    ")\n",
    "pbmc_one_shot = Benchmarkable(\n",
    "    trainer_fname=\"trainer_pbmc_one_shot\",\n",
    "    model_fname=\"model_pbmc_one_shot\",\n",
    "    name=\"Pbmc default\",\n",
    "    is_one_shot=True,\n",
    ")\n",
    "brain_large_one_shot = Benchmarkable(\n",
    "    trainer_fname=\"trainer_brain_large_one_shot\",\n",
    "    model_fname=\"model_brain_large_one_shot\",\n",
    "    name=\"Brain Large default\",\n",
    "    is_one_shot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter space `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our helper class allows us to get a dataframe per experiment resuming the results of each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cortex_df = cortex.get_param_df()\n",
    "# cortex_df.to_csv(\"cortex_df\")\n",
    "# cortex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbmc_df = pbmc.get_param_df()\n",
    "# pbmc_df.to_csv(\"pbmc_df\")\n",
    "# pbmc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain_large_df = brain_large.get_param_df()\n",
    "# brain_large_df.to_csv(\"brain_large_df\")\n",
    "# brain_large_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best run  `DataFrame `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous dataframes we are able to build one containing the best results along with the results obtained with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cortex_best = cortex_df.iloc[0]\n",
    "# cortex_best.name = \"Cortex tuned\"\n",
    "# cortex_default = pd.Series(\n",
    "#     [\n",
    "#         cortex_one_shot.best_performance,\n",
    "#         1, 128, 10, \"zinb\", 0.1, 0.001, 400, None, None\n",
    "#     ],\n",
    "#     index=cortex_best.index\n",
    "# )\n",
    "# cortex_default.name = \"Cortex default\"\n",
    "# pbmc_best = pbmc_df.iloc[0]\n",
    "# pbmc_best.name = \"Pbmc tuned\"\n",
    "# pbmc_default = pd.Series(\n",
    "#     [\n",
    "#         pbmc_one_shot.best_performance,\n",
    "#         1, 128, 10, \"zinb\", 0.1, 0.001, 400, None, None\n",
    "#     ],\n",
    "#     index=pbmc_best.index\n",
    "# )\n",
    "# pbmc_default.name = \"Pbmc default\"\n",
    "# brain_large_best = brain_large_df.iloc[0]\n",
    "# brain_large_best.name = \"Brain Large tuned\"\n",
    "# brain_large_default = pd.Series(\n",
    "#     [\n",
    "#         brain_large_one_shot.best_performance,\n",
    "#         1, 128, 10, \"zinb\", 0.1, 0.001, 400, None, None\n",
    "#     ],\n",
    "#     index=brain_large_best.index\n",
    "# )\n",
    "# brain_large_default.name = \"Brain Large default\"\n",
    "# df_best = pd.concat(\n",
    "#     [cortex_best,\n",
    "#      cortex_default,\n",
    "#      pbmc_best,\n",
    "#      pbmc_default,\n",
    "#      brain_large_best,\n",
    "#      brain_large_default\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "# df_best = df_best.iloc[np.logical_not(np.isin(df_best.index, [\"n_params\", \"run index\"]))]\n",
    "# df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handy class to compare the results of each experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a second handy class to compare these results altogether.\n",
    "Specifically, the `PlotBenchmarkable` allows to retrieve:\n",
    "* A `DataFrame` containg the runtime information of each experiment.\n",
    "* A `DataFrame` comparint the different benchmarks (negative marginal LL, imputation) between tuned and default VAEs.\n",
    "* For each dataset, a plot aggregating the ELBO histories of each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.autotune_advanced_notebook import PlotBenchmarkables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_benchmarkables = {\n",
    "#     \"cortex\": cortex,\n",
    "#     \"pbmc\": pbmc,\n",
    "#     \"brain large\": brain_large,\n",
    "# }\n",
    "# one_shot_benchmarkables = {\n",
    "#     \"cortex\": cortex_one_shot,\n",
    "#     \"pbmc\": pbmc_one_shot,\n",
    "#     \"brain large\": brain_large_one_shot\n",
    "# }\n",
    "# plotter = PlotBenchmarkables(\n",
    "#     tuned_benchmarkables=tuned_benchmarkables,\n",
    "#     one_shot_benchmarkables=one_shot_benchmarkables,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_runtime = plotter.get_runtime_dataframe()\n",
    "# df_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results `DataFrame` for best runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def highlight_min(data, color=\"yellow\"):\n",
    "#     attr = \"background-color: {}\".format(color)\n",
    "#     if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
    "#         is_min = data == data.min()\n",
    "#         return [attr if v else \"\" for v in is_min]\n",
    "#     else:  # from .apply(axis=None)\n",
    "#         is_min = data == data.min().min()\n",
    "#         return pd.DataFrame(np.where(is_min, attr, \"\"),\n",
    "#                             index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = plotter.get_results_dataframe()\n",
    "# styler = df_results.style.apply(highlight_min, axis=0, subset=pd.IndexSlice[\"cortex\", :])\n",
    "# styler = styler.apply(highlight_min, axis=0, subset=pd.IndexSlice[\"pbmc\", :])\n",
    "# styler = styler.apply(highlight_min, axis=0, subset=pd.IndexSlice[\"brain large\", :])\n",
    "# styler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Histories plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ELBO histories plotted below, the runs are colored from red to green, where red is the first run and green the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.dpi\"] = 200\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ylims_dict = {\n",
    "#     \"cortex\": [1225, 1600],\n",
    "#     \"pbmc\": [1325, 1600],\n",
    "#     \"brain large\": [140, 160],\n",
    "# }\n",
    "# plotter.plot_histories(figsize=(17, 5), ylims_dict=ylims_dict, filename=\"elbo_histories_all\", alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
